import streamlit as st
import httpx
from bs4 import BeautifulSoup
import random
import time
from fake_useragent import UserAgent

# === –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –∫–ª—ñ—î–Ω—Ç–∞ ===
@st.cache_resource
def get_client():
    return httpx.Client(
        follow_redirects=True,
        timeout=httpx.Timeout(30.0),
        verify=False  # –¥–æ–ø–æ–º–∞–≥–∞—î –æ–±—ñ–π—Ç–∏ SSL-–ø–æ–º–∏–ª–∫–∏ (443)
    )

# === –§—É–Ω–∫—Ü—ñ—è –∑–∞–ø–∏—Ç—É –∑ —Ä–µ—Ç—Ä–∞—è–º–∏ ===
def safe_request(url, retries=3):
    client = get_client()
    ua = UserAgent()
    headers = {
        "User-Agent": ua.random,
        "Accept-Language": "uk-UA,uk;q=0.9,en;q=0.8",
        "Referer": "https://www.google.com/",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    }

    for attempt in range(retries):
        try:
            time.sleep(random.uniform(1.0, 2.5))
            response = client.get(url, headers=headers)

            # –æ–±—Ä–æ–±–∫–∞ –≤—ñ–¥–æ–º–∏—Ö –±–ª–æ–∫—É–≤–∞–Ω—å
            if response.status_code in [403, 429, 503]:
                st.warning(f"{url} ‚Äî –±–ª–æ–∫ ({response.status_code}), —Å–ø—Ä–æ–±–∞ {attempt+1}")
                continue
            if any(x in response.text.lower() for x in ["cloudflare", "captcha", "checking your browser"]):
                st.warning(f"Cloudflare –±–ª–æ–∫—É—î {url}")
                continue

            return response.text

        except Exception as e:
            st.warning(f"{url} ‚Äî –ø–æ–º–∏–ª–∫–∞ {str(e)[:70]}, —Å–ø—Ä–æ–±–∞ {attempt+1}")
            time.sleep(2)

    return None

# === –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≥–æ–ª–æ–≤–∫—ñ–≤ ===
def extract_headings(html):
    soup = BeautifulSoup(html, "html.parser")
    headings = []
    for tag in ["h2", "h3", "h4"]:
        for h in soup.find_all(tag):
            text = h.get_text(strip=True)
            if (text and len(text) > 3 and 
                not any(kw in text.lower() for kw in ["footer", "cookie", "menu", "nav", "signup", "—Ä–µ—î—Å—Ç—Ä–∞—Ü—ñ—è", "–ª–æ–≥—ñ–Ω"])):
                headings.append(f"<{tag.upper()}> {text}")
    return "\n".join(headings) if headings else "‚Äî –∑–∞–≥–æ–ª–æ–≤–∫—ñ–≤ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ ‚Äî"

# === –§–æ—Ä–º—É–≤–∞–Ω–Ω—è –ø—Ä–æ–º–ø—Ç—É ===
def generate_prompt(structures, topic):
    competitors = "\n\n".join([f"–ö–æ–Ω–∫—É—Ä–µ–Ω—Ç {i+1}:\n{s}" for i, s in enumerate(structures)])
    return f"""
–°—Ç–≤–æ—Ä–∏ –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω—É SEO-—Å—Ç—Ä—É–∫—Ç—É—Ä—É (H2, H3) –¥–ª—è —Å—Ç–∞—Ç—Ç—ñ –Ω–∞ —Ç–µ–º—É **"{topic}"**, –æ—Ä—ñ—î–Ω—Ç—É—é—á–∏—Å—å –Ω–∞ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ñ–≤:

{competitors}

–í–∏–º–æ–≥–∏:
- –¢—ñ–ª—å–∫–∏ H2 (5‚Äì7 —à—Ç), H3 ‚Äî –º—ñ–Ω—ñ–º—É–º
- –ü–æ—á–∏–Ω–∞–π —ñ–∑ H2
- –ü–æ–±—É–¥—É–π –ª–æ–≥—ñ—á–Ω—É –ø–æ—Å–ª—ñ–¥–æ–≤–Ω—ñ—Å—Ç—å (–≤—Å—Ç—É–ø ‚Üí –ø–µ—Ä–µ–≤–∞–≥–∏ ‚Üí –µ—Ç–∞–ø–∏/–ø–æ—Ä–∞–¥–∏ ‚Üí —Ä–∏–∑–∏–∫–∏ ‚Üí –≤–∏—Å–Ω–æ–≤–æ–∫)
- –ë–µ–∑ FAQ, –±–µ–∑ –¥—É–±–ª—ñ–≤
- –§–æ—Ä–º–∞—Ç ‚Äî —Ç–∞–±–ª–∏—Ü—è:

| –ó–∞–≥–æ–ª–æ–≤–æ–∫ | –ü—Ä–æ —â–æ –æ–ø–∏—Å–∞—Ç–∏ |
|-----------|----------------|
| H2: ...   | ...            |
"""

# === –ó–∞–ø–∞—Å–Ω–∏–π —à–∞–±–ª–æ–Ω ===
def default_prompt():
    return """
| –ó–∞–≥–æ–ª–æ–≤–æ–∫ | –ü—Ä–æ —â–æ –æ–ø–∏—Å–∞—Ç–∏ |
|-----------|----------------|
| <H2> –©–æ —Ü–µ –∑–∞ —Ç–µ–º–∞ | –ü–æ—è—Å–Ω–∏ —Å—É—Ç–Ω—ñ—Å—Ç—å —ñ –∫–æ–Ω—Ç–µ–∫—Å—Ç |
| <H2> –û—Å–Ω–æ–≤–Ω—ñ –ø–µ—Ä–µ–≤–∞–≥–∏ | –ß–æ–º—É —Ü–µ –≤–∞—Ä—Ç–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ |
| <H2> –ù–µ–¥–æ–ª—ñ–∫–∏ –∞–±–æ —Ä–∏–∑–∏–∫–∏ | –ù–∞ —â–æ –∑–≤–µ—Ä–Ω—É—Ç–∏ —É–≤–∞–≥—É |
| <H2> –Ø–∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –≤–∏–±—Ä–∞—Ç–∏ | –ö—Ä–∏—Ç–µ—Ä—ñ—ó –∞–±–æ –ø–æ—Ä–∞–¥–∏ |
| <H2> –ü—ñ–¥—Å—É–º–∫–∏ | –ö–æ—Ä–æ—Ç–∫–∏–π –≤–∏—Å–Ω–æ–≤–æ–∫ |
"""

# === Streamlit UI ===
st.set_page_config(page_title="–ü–∞—Ä—Å–µ—Ä —Å—Ç—Ä—É–∫—Ç—É—Ä –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ñ–≤", layout="wide")
st.title("üîç SEO Parser ‚Äî –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –ø—Ä–æ–º–ø—Ç—É –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏ —Å—Ç–∞—Ç—Ç—ñ")
st.info("–ü–∞—Ä—Å–µ—Ä –Ω–∞–º–∞–≥–∞—î—Ç—å—Å—è –æ–±—ñ–π—Ç–∏ –±–ª–æ–∫—É–≤–∞–Ω–Ω—è (HTTPX + Fake UserAgent + retries)")

topic = st.text_input("–¢–µ–º–∞ –∞–±–æ –∫–ª—é—á–æ–≤–∞ —Ñ—Ä–∞–∑–∞ –¥–ª—è —Å—Ç–∞—Ç—Ç—ñ:", "–ù–∞–ø—Ä–∏–∫–ª–∞–¥: –¥–µ—Ä–µ–≤‚Äô—è–Ω—ñ –ª–∞–∑–Ω—ñ")
urls_input = st.text_area("–í—Å—Ç–∞–≤—Ç–µ URL –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ñ–≤ (–ø–æ –æ–¥–Ω–æ–º—É –Ω–∞ —Ä—è–¥–æ–∫):", height=150)

if st.button("–ü—Ä–æ–∞–Ω–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ñ–≤", type="primary"):
    urls = [u.strip() for u in urls_input.split("\n") if u.strip()]
    if not urls:
        st.error("‚ùó –î–æ–¥–∞–π—Ç–µ —Ö–æ—á–∞ –± –æ–¥–∏–Ω URL.")
        st.stop()

    results = []
    progress = st.progress(0)

    for i, url in enumerate(urls):
        with st.spinner(f"–ü–∞—Ä—Å–∏–º–æ {i+1}/{len(urls)}: {url}"):
            html = safe_request(url)
            if html:
                structure = extract_headings(html)
                results.append(structure)
            else:
                results.append("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –æ—Ç—Ä–∏–º–∞—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç.")
        progress.progress((i + 1) / len(urls))

    if any("H2" in r or "H3" in r for r in results):
        prompt = generate_prompt(results, topic)
        col1, col2 = st.columns(2)
        with col1:
            st.subheader("–°—Ç—Ä—É–∫—Ç—É—Ä–∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ñ–≤")
            st.code("\n\n".join([f"–ö–æ–Ω–∫—É—Ä–µ–Ω—Ç {i+1}:\n{s}" for i, s in enumerate(results)]))
        with col2:
            st.subheader("–ü—Ä–æ–º–ø—Ç –¥–ª—è –∫–æ–ø—ñ—Ä–∞–π—Ç–µ—Ä–∞")
            st.code(prompt, language="markdown")
    else:
        st.warning("–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–Ω–∞–π—Ç–∏ –∂–æ–¥–Ω–æ–≥–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞.")
        st.code(default_prompt(), language="markdown")
